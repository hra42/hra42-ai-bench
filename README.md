# HRA42 AI Bench - Advanced LLM Benchmarking Platform

[![SvelteKit](https://img.shields.io/badge/SvelteKit-FF3E00?style=for-the-badge&logo=svelte&logoColor=white)](https://kit.svelte.dev)
[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=for-the-badge&logo=typescript&logoColor=white)](https://www.typescriptlang.org)
[![DuckDB](https://img.shields.io/badge/DuckDB-FFF000?style=for-the-badge&logo=duckdb&logoColor=black)](https://duckdb.org)
[![Tailwind CSS](https://img.shields.io/badge/Tailwind_CSS-38B2AC?style=for-the-badge&logo=tailwind-css&logoColor=white)](https://tailwindcss.com)

## ğŸš€ Overview

HRA42 AI Bench is a comprehensive benchmarking platform for comparing Large Language Model (LLM) performance across multiple dimensions. Built with modern web technologies, it provides real-time comparison of various AI models through OpenRouter's unified API, with detailed metrics tracking and cost analysis.

### âœ¨ Key Features

- **ğŸ¯ Multi-Modal Benchmarking**: Test models across text generation, structured output, vision analysis, and document processing
- **âš¡ Real-Time Comparison**: Execute benchmarks on multiple models simultaneously with live streaming results
- **ğŸ’° Cost Tracking**: Detailed cost analysis per model, per token, and per benchmark run
- **ğŸ“Š Performance Analytics**: Comprehensive metrics including response time, token usage, and quality scores
- **ğŸ—„ï¸ Historical Analysis**: Track model performance over time with trend detection
- **ğŸ¨ Modern UI**: Responsive, accessible interface built with Tailwind CSS
- **ğŸ”’ Privacy-First**: All data stored locally using embedded DuckDB database

## ğŸ“‹ Table of Contents

- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Features](#-features)
- [Architecture](#-architecture)
- [API Documentation](#-api-documentation)
- [User Guide](#-user-guide)
- [Development](#-development)
- [Contributing](#-contributing)
- [License](#-license)

## ğŸ› ï¸ Installation

### Prerequisites

- Node.js 18+ 
- npm or pnpm
- OpenRouter API key ([Get one here](https://openrouter.ai))

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/hra42/hra42-ai-bench.git
cd hra42-ai-bench
```

2. **Install dependencies**
```bash
npm install
```

3. **Configure environment variables**
```bash
cp .env.example .env
```

Edit `.env` and add your OpenRouter API key:
```env
OPENROUTER_API_KEY=your-api-key-here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
DATABASE_PATH=data/hra42.duckdb
```

4. **Initialize the database**
```bash
npm run db:init
```

5. **Start the development server**
```bash
npm run dev
```

Visit [http://localhost:5173](http://localhost:5173) to access the application.

## ğŸš€ Quick Start

### Running Your First Benchmark

1. **Navigate to the Benchmark page** from the main dashboard
2. **Select benchmark type** (Text, Structured, Vision, etc.)
3. **Choose models** to compare from the dropdown
4. **Configure your prompt** or upload files
5. **Click "Execute Benchmark"** to start
6. **View real-time results** as they stream in

### Example: Text Generation Benchmark

```typescript
// Example prompt configuration
const benchmark = {
  type: 'text',
  prompt: 'Explain quantum computing in simple terms',
  models: ['gpt-4', 'claude-3-opus', 'gemini-pro'],
  temperature: 0.7,
  maxTokens: 500
}
```

## ğŸ¯ Features

### Benchmark Types

#### ğŸ“ Text Generation
Compare how different models respond to the same prompt with detailed token usage and cost analysis.

#### ğŸ”§ Structured Output
Test models' ability to generate valid JSON according to specified schemas.

#### ğŸ› ï¸ Function Calling
Evaluate how well models understand and execute function definitions.

#### ğŸ–¼ï¸ Vision Analysis
Compare image understanding capabilities across vision-enabled models.

#### ğŸ“„ Document Processing
Test PDF processing and document understanding abilities.

### Advanced Features

#### ğŸ”„ Real-Time Streaming
- Server-Sent Events for live updates
- Progressive response rendering
- Concurrent model execution

#### ğŸ“Š Analytics Dashboard
- Cost per token analysis
- Response time comparison
- Quality metrics tracking
- Historical trend analysis

#### ğŸ’¾ Data Management
- Export results to CSV/JSON
- Save benchmark templates
- Historical data browsing
- Batch result comparison

## ğŸ—ï¸ Architecture

### Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| Frontend | SvelteKit | Full-stack framework |
| Styling | Tailwind CSS v4 | Utility-first CSS |
| Database | DuckDB | Embedded analytics |
| API | OpenRouter | Unified LLM access |
| Language | TypeScript | Type safety |
| State | Svelte Stores | Reactive state management |

### Project Structure

```
hra42-ai-bench/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ routes/              # Pages and API endpoints
â”‚   â”‚   â”œâ”€â”€ benchmark/       # Main benchmarking interface
â”‚   â”‚   â”œâ”€â”€ history/         # Historical results viewer
â”‚   â”‚   â””â”€â”€ api/            # Backend API routes
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ server/         # Server-side code
â”‚   â”‚   â”‚   â”œâ”€â”€ db/         # DuckDB integration
â”‚   â”‚   â”‚   â””â”€â”€ openrouter/ # API client
â”‚   â”‚   â”œâ”€â”€ components/     # UI components (atomic design)
â”‚   â”‚   â”‚   â”œâ”€â”€ atoms/      # Basic components
â”‚   â”‚   â”‚   â”œâ”€â”€ molecules/  # Composed components
â”‚   â”‚   â”‚   â”œâ”€â”€ organisms/  # Complex sections
â”‚   â”‚   â”‚   â””â”€â”€ templates/  # Page layouts
â”‚   â”‚   â”œâ”€â”€ stores/         # State management
â”‚   â”‚   â”œâ”€â”€ types/          # TypeScript definitions
â”‚   â”‚   â””â”€â”€ utils/          # Helper functions
â”‚   â””â”€â”€ app.html            # App shell
â”œâ”€â”€ static/                 # Static assets
â”œâ”€â”€ tests/                  # Test files
â””â”€â”€ data/                   # Database storage
```

### Database Schema

```sql
-- Core tables
CREATE TABLE benchmark_runs (
    id TEXT PRIMARY KEY,
    type TEXT NOT NULL,
    prompt TEXT,
    config JSON,
    status TEXT,
    created_at TIMESTAMP,
    completed_at TIMESTAMP
);

CREATE TABLE model_responses (
    id TEXT PRIMARY KEY,
    run_id TEXT REFERENCES benchmark_runs(id),
    model TEXT NOT NULL,
    response TEXT,
    tokens_used INTEGER,
    cost DECIMAL(10,6),
    latency_ms INTEGER,
    error TEXT,
    metadata JSON
);

CREATE TABLE models (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    provider TEXT,
    context_length INTEGER,
    pricing JSON,
    capabilities JSON,
    updated_at TIMESTAMP
);
```

## ğŸ“š API Documentation

### Core Endpoints

#### `GET /api/models`
Fetch available models from OpenRouter.

**Response:**
```json
{
  "models": [
    {
      "id": "openai/gpt-4",
      "name": "GPT-4",
      "contextLength": 8192,
      "pricing": {
        "prompt": 0.03,
        "completion": 0.06
      }
    }
  ]
}
```

#### `POST /api/execute`
Execute a benchmark run.

**Request:**
```json
{
  "type": "text",
  "models": ["gpt-4", "claude-3"],
  "prompt": "Your prompt here",
  "config": {
    "temperature": 0.7,
    "maxTokens": 500
  }
}
```

#### `GET /api/execute/stream`
Stream benchmark execution results via Server-Sent Events.

#### `GET /api/results/:runId`
Fetch results for a specific benchmark run.

#### `GET /api/history`
Retrieve historical benchmark data with filtering options.

#### `POST /api/export`
Export benchmark results in various formats (CSV, JSON, Excel).

## ğŸ“– User Guide

### Workflow Examples

#### Comparing Model Creativity
1. Select "Text Generation" benchmark type
2. Choose creative writing models
3. Set temperature to 0.9 for more creative outputs
4. Enter a creative prompt
5. Compare responses for originality and coherence

#### Testing JSON Generation
1. Select "Structured Output" benchmark
2. Define your JSON schema
3. Choose models with JSON mode support
4. Execute and validate schema compliance

#### Analyzing Images
1. Select "Vision Analysis" benchmark
2. Upload image (JPEG, PNG, WebP)
3. Choose vision-capable models
4. Add analysis prompt
5. Compare interpretation accuracy

### Best Practices

- **Model Selection**: Choose 3-5 models for optimal comparison
- **Prompt Engineering**: Use clear, specific prompts for consistent results
- **Cost Management**: Monitor token usage and set limits
- **Template Usage**: Save successful benchmark configurations
- **Export Strategy**: Regular exports for long-term analysis

## ğŸ”§ Development

### Commands

```bash
# Development
npm run dev        # Start dev server with hot reload
npm run build      # Build for production
npm run preview    # Preview production build

# Code Quality
npm run check      # Type checking
npm run lint       # Linting and formatting check
npm run format     # Auto-format code

# Database
npm run db:init    # Initialize database
npm run db:migrate # Run migrations
npm run db:seed    # Seed sample data

# Testing
npm run test       # Run tests
npm run test:ui    # Run tests with UI
```

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENROUTER_API_KEY` | OpenRouter API key | Required |
| `OPENROUTER_BASE_URL` | API base URL | `https://openrouter.ai/api/v1` |
| `DATABASE_PATH` | DuckDB file location | `data/hra42.duckdb` |
| `PUBLIC_APP_NAME` | Application name | `HRA42 AI Bench` |
| `PUBLIC_MAX_FILE_SIZE` | Max upload size | `20971520` (20MB) |

### Component Development

All components follow atomic design principles:

```svelte
<!-- Example: atoms/Button.svelte -->
<script lang="ts">
  export let variant: 'primary' | 'secondary' = 'primary';
  export let size: 'sm' | 'md' | 'lg' = 'md';
</script>

<button
  class="px-4 py-2 rounded-lg transition-colors
         {variant === 'primary' ? 'bg-blue-600 text-white hover:bg-blue-700' : ''}
         {size === 'sm' ? 'text-sm' : ''}"
  on:click
>
  <slot />
</button>
```

### Adding New Benchmark Types

1. Define type in `src/lib/types/benchmark.ts`
2. Create UI component in `src/lib/components/organisms/`
3. Add execution logic in `src/routes/api/execute/+server.ts`
4. Update result display in `src/lib/components/organisms/ModelResponseCard.svelte`

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Process

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Style

- Follow TypeScript best practices
- Use Tailwind utility classes exclusively
- Write comprehensive tests
- Document complex logic
- Keep components small and focused

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [OpenRouter](https://openrouter.ai) for unified LLM API access
- [SvelteKit](https://kit.svelte.dev) for the amazing framework
- [DuckDB](https://duckdb.org) for the powerful embedded database
- [Tailwind CSS](https://tailwindcss.com) for the utility-first CSS framework

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/hra42/hra42-ai-bench/issues)
- **Discussions**: [GitHub Discussions](https://github.com/hra42/hra42-ai-bench/discussions)
- **Email**: support@hra42.com

---

Built with â¤ï¸ by the HRA42 team